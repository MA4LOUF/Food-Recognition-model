{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkR9L3gZ1O4w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image, ImageFilter\n",
        "from PIL.Image import Resampling as ImageResampling\n",
        "import numpy as np\n",
        "\n",
        "def process_image(image_path, output_folder, category_id, categories, size=(224, 224), sharpness_threshold=100):\n",
        "        try:\n",
        "            with Image.open(image_path) as img:\n",
        "                img = img.resize(size, ImageResampling.LANCZOS)\n",
        "                rgb_img = img.convert('RGB')\n",
        "                category_folder = os.path.join(output_folder, categories.get(category_id, 'Unknown'))\n",
        "                if not os.path.exists(category_folder):\n",
        "                    os.makedirs(category_folder)\n",
        "                output_path = os.path.join(category_folder, os.path.basename(image_path))\n",
        "                rgb_img.save(output_path, 'JPEG')\n",
        "                print(f\"Processed and moved {os.path.basename(output_path)} to {category_folder}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process image {image_path}: {str(e)}\")\n",
        "\n",
        "def load_categories(category_file):\n",
        "    category_dict = {}\n",
        "    with open(category_file, 'r') as file:\n",
        "        next(file)  # Skip the header line\n",
        "        for line in file:\n",
        "            category_id, category_name = line.strip().split('\\t')\n",
        "            category_dict[int(category_id)] = category_name\n",
        "    return category_dict\n",
        "\n",
        "categories = load_categories('C:\\\\Users\\\\User\\\\Desktop\\\\ML Project\\\\category.txt')\n",
        "input_folder = 'C:\\\\Users\\\\User\\\\Desktop\\\\ML Project\\\\data'\n",
        "output_folder = os.path.join('C:\\\\Users\\\\User\\Desktop\\\\ML Project', 'Cleaned data')\n",
        "\n",
        "# Ensure the output directory exists\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Iterate through each category directory\n",
        "for category_id in range(1, 10):  # Assuming categories are numbered 1 through 9\n",
        "    category_path = os.path.join(input_folder, str(category_id))\n",
        "    if os.path.exists(category_path):\n",
        "        for image_file in os.listdir(category_path):\n",
        "            if image_file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                image_path = os.path.join(category_path, image_file)\n",
        "                process_image(image_path, output_folder, category_id, categories)\n",
        "    else:\n",
        "        print(f\"Category folder {category_path} does not exist.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct5eU4rq1k5v",
        "outputId": "924c5afa-39af-4cda-d240-c9deaa56b978"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "from torchvision import transforms, datasets, models\n",
        "import numpy as np\n",
        "\n",
        "class ModifiedResNet(nn.Module):\n",
        "    def __init__(self, num_classes=9):\n",
        "        super(ModifiedResNet, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        num_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "# Define data augmentation and normalization for training\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
        "])\n",
        "\n",
        "# Define the base directory where your dataset is located\n",
        "base_dir = 'C:\\\\Users\\\\User\\\\Desktop\\\\ML Project\\\\Cleaned data'\n",
        "\n",
        "# Load the dataset with ImageFolder\n",
        "dataset = datasets.ImageFolder(root=base_dir, transform=transform)\n",
        "\n",
        "# Calculate class weights\n",
        "class_counts = torch.tensor([sum(np.array(dataset.targets) == i) for i in range(9)])\n",
        "class_weights = 1.0 / class_counts.float()\n",
        "sample_weights = class_weights[dataset.targets]\n",
        "\n",
        "# Define oversampling and undersampling factors\n",
        "oversampling_factor = 1.5  # Increase this value for more oversampling\n",
        "undersampling_factor = 0.5  # Decrease this value for more undersampling\n",
        "\n",
        "# Apply oversampling and undersampling by adjusting sample weights\n",
        "for i in range(len(dataset)):\n",
        "    class_weight = class_weights[dataset.targets[i]]\n",
        "    sample_weights[i] *= oversampling_factor if class_weight < 1.0 else undersampling_factor\n",
        "\n",
        "# Create a WeightedRandomSampler\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = int(0.1 * len(dataset))\n",
        "validation_size = len(dataset) - train_size - test_size\n",
        "train_dataset, test_dataset, validation_dataset = random_split(dataset, [train_size, test_size, validation_size])\n",
        "\n",
        "# Setup data loaders with balanced sampling\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, sampler=sampler)\n",
        "validation_loader = DataLoader(dataset=validation_dataset, batch_size=32)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32)\n",
        "\n",
        "# Initialize the network\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ModifiedResNet(num_classes=9).to(device)\n",
        "\n",
        "# Loss function with class weights\n",
        "class_weights = class_weights.to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer (you can choose any optimizer)\n",
        "optimizer = optim.Adam(model.resnet.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Print some information (optional)\n",
        "print(f\"Total images in dataset: {len(dataset)}\")\n",
        "print(f\"Images in training set: {train_size}\")\n",
        "print(f\"Images in validation set: {validation_size}\")\n",
        "print(f\"Images in testing set: {test_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "1garmToD1tlw",
        "outputId": "a999bcb6-fada-4cd6-9c19-d5095e6175f8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "\n",
        "def evaluate_model(model, loader, device, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * images.size(0)  # Accumulate the total loss\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()  # Count the number of correct predictions\n",
        "            total_predictions += labels.size(0)  # Count the total number of predictions\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    average_loss = total_loss / len(loader.dataset)\n",
        "    accuracy = (correct_predictions / total_predictions) * 100\n",
        "\n",
        "    return average_loss, accuracy\n",
        "\n",
        "\n",
        "def grid_search_and_save(model, params, train_loader, validation_loader, num_epochs=15, save_path=''):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    param_combinations = list(itertools.product(*params.values()))\n",
        "    best_accuracy = 0\n",
        "    best_params = {}\n",
        "\n",
        "    for combination in param_combinations:\n",
        "        param_dict = dict(zip(params.keys(), combination))\n",
        "        print(f\"Testing combination: {param_dict}\")\n",
        "        model.resnet.fc.reset_parameters()\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=param_dict['lr'])\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            train_total_loss = 0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_total_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "                train_total += labels.size(0)\n",
        "            train_avg_loss = train_total_loss / train_total\n",
        "            train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "            val_loss, val_accuracy = evaluate_model(model, validation_loader, device, criterion)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, LR={param_dict['lr']}, Batch Size={param_dict['batch_size']}: Train Loss {train_avg_loss:.4f}, Train Acc {train_accuracy:.2f}%, Val Loss {val_loss:.4f}, Val Acc {val_accuracy:.2f}%\")\n",
        "\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "                best_params = param_dict\n",
        "\n",
        "    if save_path:\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        print(f\"Best hyperparameters: {best_params}\")\n",
        "        print(f\"Best validation accuracy: {best_accuracy:.2f}%\")\n",
        "        # No model weights are saved in this version\n",
        "\n",
        "    return best_params, best_accuracy\n",
        "\n",
        "\n",
        "# Define hyperparameters for grid search\n",
        "params = {\n",
        "    'lr': [0.001],\n",
        "    'batch_size': [32, 64, 128]  # Add batch size options here\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define hyperparameters for grid search\n",
        "params = {\n",
        "    'lr': [0.001,0.01 ,0.1],\n",
        "    'batch_size': [32, 64, 128]  # Add batch size options here\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "model = ModifiedResNet(num_classes=9)\n",
        "\n",
        "# Define data loaders for training and validation sets\n",
        "# Note: Make sure to adjust batch size and other parameters as needed\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "validation_loader = DataLoader(dataset=validation_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Specify the number of epochs for training\n",
        "num_epochs = 15\n",
        "\n",
        "# Specify the path where you want to save the best model weights\n",
        "save_path = 'C:\\\\Users\\\\User\\\\Desktop\\\\ML Project\\\\best_model'\n",
        "\n",
        "# Perform grid search and train the model\n",
        "best_params, best_accuracy = grid_search_and_save(model, params, train_loader, validation_loader, num_epochs, save_path)\n",
        "\n",
        "# Print the best hyperparameters and validation accuracy\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Validation Accuracy:\", best_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, validation_loader, optimizer, criterion, device, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate average train loss and accuracy for the epoch\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_accuracy = 100 * correct / total\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        val_loss, val_accuracy = evaluate_model(model, validation_loader, device, criterion)\n",
        "\n",
        "        # Print the epoch statistics\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
        "\n",
        "    print('Training finished.')\n",
        "\n",
        "# Define the evaluate_model function as before\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the network with the best hyperparameters\n",
        "model = ModifiedResNet(num_classes=9).to(device)\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load('C:\\\\Users\\\\User\\\\Desktop\\\\ML Project\\\\best_model\\\\best_model_weights.pth'))\n",
        "\n",
        "# Define the optimizer with the selected learning rate\n",
        "optimizer = optim.Adam(model.resnet.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model using the entire training dataset\n",
        "# You can adjust the number of epochs and other parameters as needed\n",
        "train_model(model, train_loader, validation_loader, optimizer, criterion, device, num_epochs=15)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = evaluate_model(model, test_loader, device, criterion)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGx32JHj1udq",
        "outputId": "f0133ee2-7693-4621-ada1-9393f81979dd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to show image and prediction\n",
        "def show_prediction(model, loader, device, num_samples=5):\n",
        "    model.eval()\n",
        "    classes = loader.dataset.dataset.classes  # Access classes from the original dataset\n",
        "    with torch.no_grad():\n",
        "        for i, (images, labels) in enumerate(loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            plt.figure(figsize=(15, 5))  # Increase figure size\n",
        "            for j in range(min(num_samples, images.size(0))):  # Limit to num_samples or batch size\n",
        "                plt.subplot(1, num_samples, j+1)\n",
        "                image = images[j].cpu().numpy().transpose((1, 2, 0))\n",
        "                plt.imshow(image)\n",
        "                plt.title(f'Predicted: {classes[predicted[j].item()]}\\nActual: {classes[labels[j].item()]}', fontsize=10)  # Adjust fontsize\n",
        "                plt.axis('off')\n",
        "            plt.tight_layout()  # Adjust spacing between subplots\n",
        "            plt.show()\n",
        "            break  # Show only one batch of images\n",
        "\n",
        "# Call the function to show predictions\n",
        "show_prediction(model, validation_loader, device)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
